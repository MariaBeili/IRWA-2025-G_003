{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21005e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: data/fashion_products_dataset.json\n",
      "Rows loaded: 28080\n",
      "Using a 20% sample → 5616 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 5616/5616 [01:45<00:00, 53.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed: 5616 / 5616 (Errors: 0)\n",
      "Final DataFrame: (5616, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_9764\\2074399715.py:243: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=top_brands.values, y=top_brands.index, palette=\"Blues_r\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA completed. Results saved in: outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from types import SimpleNamespace\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# DATA PREPARATION \n",
    "# ==============================\n",
    "class ProcessedDocument(BaseModel):\n",
    "    _id: Optional[str]\n",
    "    pid: str\n",
    "    title: str\n",
    "    description: Optional[str]\n",
    "    brand: Optional[str]\n",
    "    category: Optional[str]\n",
    "    sub_category: Optional[str]\n",
    "    product_details: Optional[Dict[str, Any]]\n",
    "    seller: Optional[str]\n",
    "    out_of_stock: Optional[bool]\n",
    "    selling_price: Optional[float]\n",
    "    discount: Optional[float]\n",
    "    actual_price: Optional[float]\n",
    "    average_rating: Optional[float]\n",
    "    url: Optional[str]\n",
    "\n",
    "    title_processed: Optional[List[str]] = None\n",
    "    description_processed: Optional[List[str]] = None\n",
    "    brand_processed: Optional[List[str]] = None\n",
    "    category_processed: Optional[List[str]] = None\n",
    "    sub_category_processed: Optional[List[str]] = None\n",
    "    seller_processed: Optional[List[str]] = None\n",
    "    product_details_processed: Optional[Dict[str, Any]] = None\n",
    "    search_text: Optional[List[str]] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_document(cls, doc) -> \"ProcessedDocument\":\n",
    "        return cls(\n",
    "            _id=getattr(doc, \"_id\", None),\n",
    "            pid=doc.pid,\n",
    "            title=doc.title,\n",
    "            description=doc.description,\n",
    "            brand=doc.brand,\n",
    "            category=doc.category,\n",
    "            sub_category=doc.sub_category,\n",
    "            product_details=doc.product_details,\n",
    "            seller=doc.seller,\n",
    "            out_of_stock=doc.out_of_stock,\n",
    "            selling_price=doc.selling_price,\n",
    "            discount=doc.discount,\n",
    "            actual_price=doc.actual_price,\n",
    "            average_rating=doc.average_rating,\n",
    "            url=doc.url\n",
    "        )\n",
    "\n",
    "    def process_fields(self):\n",
    "        self.title_processed = self._process_text(self.title)\n",
    "        self.description_processed = self._process_text(self.description)\n",
    "        self.brand_processed = self._process_text(self.brand)\n",
    "        self.category_processed = self._process_text(self.category)\n",
    "        self.sub_category_processed = self._process_text(self.sub_category)\n",
    "        self.seller_processed = self._process_text(self.seller)\n",
    "        self.product_details_processed = self._process_product_details()\n",
    "        self.search_text = self._combine_search_text()\n",
    "\n",
    "    def _process_text(self, text: Optional[str]) -> List[str]:\n",
    "        if not text:\n",
    "            return []\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "        except LookupError:\n",
    "            nltk.download(\"punkt\", quiet=True)\n",
    "            tokens = word_tokenize(text)\n",
    "        try:\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "        except LookupError:\n",
    "            nltk.download(\"stopwords\", quiet=True)\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "        return tokens\n",
    "\n",
    "    def _process_product_details(self) -> Dict[str, Any]:\n",
    "        if not self.product_details:\n",
    "            return {}\n",
    "        processed = {}\n",
    "        for k, v in self.product_details.items():\n",
    "            processed[k] = self._process_text(str(v))\n",
    "        return processed\n",
    "\n",
    "    def _combine_search_text(self) -> List[str]:\n",
    "        parts = []\n",
    "        for field in [\n",
    "            self.title_processed, self.description_processed, self.brand_processed,\n",
    "            self.category_processed, self.sub_category_processed, self.seller_processed\n",
    "        ]:\n",
    "            if field:\n",
    "                parts += field\n",
    "        if self.product_details_processed:\n",
    "            for v in self.product_details_processed.values():\n",
    "                parts += v\n",
    "        return parts\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# EXPLORATORY ANALYSIS\n",
    "# ==============================\n",
    "def parse_numeric(value):\n",
    "    \"\"\"Convert values such as '1,499' or '55% off' into clean float values.\"\"\"\n",
    "    if value is None or (isinstance(value, float) and np.isnan(value)):\n",
    "        return None\n",
    "    if isinstance(value, (int, float)):\n",
    "        return float(value)\n",
    "    s = str(value)\n",
    "    s = re.sub(r\"[^\\d\\.]\", \"\", s)\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def normalize_product_details(details):\n",
    "    \"\"\"Convert lists of dictionaries into a single merged dictionary.\"\"\"\n",
    "    if isinstance(details, dict):\n",
    "        return details\n",
    "    if isinstance(details, list):\n",
    "        merged = {}\n",
    "        for d in details:\n",
    "            if isinstance(d, dict):\n",
    "                merged.update(d)\n",
    "        return merged\n",
    "    return {}\n",
    "\n",
    "\n",
    "def run_exploration(data_path: str, outdir: str, sample_frac: float = 0.2):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    print(f\"Loading data from: {data_path}\")\n",
    "    df = pd.read_json(data_path)\n",
    "    print(\"Rows loaded:\", len(df))\n",
    "\n",
    "    # Sampling to speed up the process (set 1.0 to use the full dataset)\n",
    "    if sample_frac < 1.0:\n",
    "        df = df.sample(frac=sample_frac, random_state=42).reset_index(drop=True)\n",
    "        print(f\"Using a {int(sample_frac * 100)}% sample → {len(df)} rows\")\n",
    "\n",
    "    processed_docs = []\n",
    "    errors = 0\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing documents\"):\n",
    "        try:\n",
    "            rdict = row.to_dict()\n",
    "\n",
    "            # Normalization\n",
    "            rdict[\"product_details\"] = normalize_product_details(rdict.get(\"product_details\"))\n",
    "            for field in [\"selling_price\", \"actual_price\", \"discount\", \"average_rating\"]:\n",
    "                rdict[field] = parse_numeric(rdict.get(field))\n",
    "\n",
    "            doc_obj = SimpleNamespace(**rdict)\n",
    "            pdoc = ProcessedDocument.from_document(doc_obj)\n",
    "            pdoc.process_fields()\n",
    "            processed_docs.append(pdoc)\n",
    "\n",
    "        except Exception:\n",
    "            errors += 1\n",
    "            continue\n",
    "\n",
    "    print(f\"Successfully processed: {len(processed_docs)} / {len(df)} (Errors: {errors})\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    data = []\n",
    "    for d in processed_docs:\n",
    "        dct = d.model_dump()\n",
    "        data.append({\n",
    "            \"pid\": dct.get(\"pid\"),\n",
    "            \"brand\": dct.get(\"brand\"),\n",
    "            \"category\": dct.get(\"category\"),\n",
    "            \"seller\": dct.get(\"seller\"),\n",
    "            \"out_of_stock\": dct.get(\"out_of_stock\"),\n",
    "            \"selling_price\": dct.get(\"selling_price\"),\n",
    "            \"discount\": dct.get(\"discount\"),\n",
    "            \"actual_price\": dct.get(\"actual_price\"),\n",
    "            \"average_rating\": dct.get(\"average_rating\"),\n",
    "            \"token_count\": len(dct.get(\"search_text\", []))\n",
    "        })\n",
    "\n",
    "    df_proc = pd.DataFrame(data)\n",
    "    print(\"Final DataFrame:\", df_proc.shape)\n",
    "\n",
    "    # ==========================\n",
    "    #  ANALYSIS & VISUALS\n",
    "    # ==========================\n",
    "    summary = {\n",
    "        \"n_docs\": len(df_proc),\n",
    "        \"n_brands\": df_proc[\"brand\"].nunique(),\n",
    "        \"n_categories\": df_proc[\"category\"].nunique(),\n",
    "        \"avg_price\": round(df_proc[\"selling_price\"].mean(), 2),\n",
    "        \"avg_discount\": round(df_proc[\"discount\"].mean(), 2),\n",
    "        \"avg_rating\": round(df_proc[\"average_rating\"].mean(), 2),\n",
    "        \"out_of_stock_ratio\": round(df_proc[\"out_of_stock\"].mean(), 3)\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(outdir, \"summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(df_proc[\"selling_price\"].dropna(), bins=40, color=\"steelblue\")\n",
    "    plt.title(\"Selling Price Distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"price_distribution.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(df_proc[\"average_rating\"].dropna(), bins=20, color=\"darkorange\")\n",
    "    plt.title(\"Average Rating Distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"rating_distribution.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    top_brands = df_proc[\"brand\"].value_counts().head(15)\n",
    "    sns.barplot(x=top_brands.values, y=top_brands.index, palette=\"Blues_r\")\n",
    "    plt.title(\"Top 15 Brands\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"top_brands.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    stock_counts = df_proc[\"out_of_stock\"].value_counts()\n",
    "    stock_counts.index = [\"In Stock\", \"Out of Stock\"]\n",
    "    stock_counts.plot(kind=\"pie\", autopct=\"%1.1f%%\", colors=[\"#8fd9b6\", \"#f6a6a6\"])\n",
    "    plt.title(\"Stock Availability\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"stock_pie.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    all_tokens = []\n",
    "    for d in processed_docs:\n",
    "        all_tokens += d.search_text or []\n",
    "    vocab = Counter(all_tokens)\n",
    "    wc = WordCloud(width=1000, height=500, background_color=\"white\").generate_from_frequencies(vocab)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Most Frequent Words (WordCloud)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"wordcloud.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    print(\"EDA completed. Results saved in:\", outdir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Simple configuration without argparse\n",
    "    data_path = \"data/fashion_products_dataset.json\"\n",
    "    outdir = \"outputs\"\n",
    "    run_exploration(data_path, outdir, sample_frac=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca68a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
