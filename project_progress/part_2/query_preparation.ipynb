{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34ec25",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02fc07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(line: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Preprocess the article line (title + body) by:\n",
    "    - Removing accents (é, ö, etc.)\n",
    "    - Removing stop words\n",
    "    - Stemming\n",
    "    - Lowercasing\n",
    "    - Tokenizing\n",
    "    \n",
    "    Argument:\n",
    "    line -- string to be preprocessed\n",
    "\n",
    "    Returns:\n",
    "    tokens -- a list of tokens corresponding to the input line after preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    if not line:\n",
    "        return []\n",
    "    \n",
    "    line = unicodedata.normalize(\"NFKD\", line).encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "\n",
    "    # Lowercase\n",
    "    line = line.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    line = re.sub(r\"[^\\w\\s]\", \" \", line)\n",
    "\n",
    "    # Tokenize\n",
    "    try:\n",
    "        tokens = word_tokenize(line)\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "        nltk.download(\"punkt_tab\", quiet=True)\n",
    "        tokens = word_tokenize(line)\n",
    "\n",
    "    # Loading stopwords\n",
    "    try:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "    except LookupError:\n",
    "        nltk.download(\"stopwords\", quiet=True)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    return tokens"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
