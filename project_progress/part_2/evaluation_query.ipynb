{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the project root to the path so we can import our modules\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6843cd0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Here we evaluate the two given queries against a custom validation set stored in 'validation_labels.csv'.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2223e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your TF-IDF search\n",
    "from indexing import search_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01701196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation metrics\n",
    "from evaluation import (\n",
    "    compute_precision_at_K,\n",
    "    compute_recall_at_K,\n",
    "    compute_average_precision_at_K,\n",
    "    compute_F1_score_at_K,\n",
    "    compute_mean_average_precision,\n",
    "    compute_mean_reciprocal_rank,\n",
    "    compute_normalized_discounted_cumulative_gain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e6d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a545542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Helper Functions\n",
    "# -------------------------------------------------------------\n",
    "def load_index_data(path: str) -> dict:\n",
    "    # Loads the precomputed inverted index and TF-IDF data\n",
    "    print(f\"Loading index from {path}...\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    print(\"Index loaded successfully.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f618e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baabc33",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_ground_truth(path: str) -> dict:\n",
    "    # Reads the CSV file with validation labels\n",
    "    print(f\"Loading ground truth from {path}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Ground truth file not found at {path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Map query IDs to text\n",
    "    query_map = {\n",
    "        1: \"women full sleeve sweatshirt cotton\",\n",
    "        2: \"men slim jeans blue\"\n",
    "    }\n",
    "    df['query'] = df['query_id'].map(query_map)\n",
    "\n",
    "    # Group by query and collect relevant PIDs\n",
    "    ground_truth = df.groupby('query').apply(\n",
    "        lambda x: set(x[x['labels'] == 1]['pid'])\n",
    "    ).to_dict()\n",
    "\n",
    "    print(f\"Loaded ground truth for {len(ground_truth)} queries.\")\n",
    "    return ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(all_rankings, all_relevants, K):\n",
    "    \"\"\"Mean Average Precision across queries.\"\"\"\n",
    "    total = 0\n",
    "    for ranking, rel in zip(all_rankings, all_relevants):\n",
    "        total += compute_average_precision_at_K(ranking, rel, K)\n",
    "    return total / len(all_rankings) if all_rankings else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbae4f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def mrr(all_rankings, all_relevants):\n",
    "    \"\"\"Mean Reciprocal Rank across queries.\"\"\"\n",
    "    total = 0\n",
    "    for ranking, rel in zip(all_rankings, all_relevants):\n",
    "        for idx, doc in enumerate(ranking, start=1):\n",
    "            if doc in rel:\n",
    "                total += 1 / idx\n",
    "                break\n",
    "    return total / len(all_rankings) if all_rankings else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f20f5e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Main Evaluation\n",
    "# -------------------------------------------------------------\n",
    "def main():\n",
    "    # File paths and configuration\n",
    "    INDEX_PATH = \"project_progress/part_2/irwa_index.pkl\"\n",
    "    LABELS_PATH = \"data/validation_labels.csv\"\n",
    "    K = 10  # Metrics will be computed @10\n",
    "\n",
    "    # Loading data\n",
    "    try:\n",
    "        index_data = load_index_data(INDEX_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Index file not found at {INDEX_PATH}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    \n",
    "    ground_truth = load_ground_truth(LABELS_PATH)\n",
    "\n",
    "    index = index_data['index']\n",
    "    tf = index_data['tf']\n",
    "    idf = index_data['idf']\n",
    "\n",
    "    # Queries to evaluate\n",
    "    queries = [\n",
    "        \"women full sleeve sweatshirt cotton\",\n",
    "        \"men slim jeans blue\"\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    all_ground_truths = []\n",
    "    query_metrics = {q: {} for q in queries}\n",
    "\n",
    "    print(f\"\\n--- Evaluation Results (K={K}) ---\")\n",
    "    print(\"=\" * 44)\n",
    "    print(f\"{'Metric':<8} | {'Query 1 (women...)':<18} | {'Query 2 (men...)':<15}\")\n",
    "    print(\"-\" * 44)\n",
    "\n",
    "    for query in queries:\n",
    "        ranked_pids = search_tfidf(query, index, tf, idf)\n",
    "        relevant_set = ground_truth.get(query, set())\n",
    "\n",
    "        all_results.append(ranked_pids)\n",
    "        all_ground_truths.append(relevant_set)\n",
    "\n",
    "        # Convert sets to lists because evaluation functions expect ordered lists\n",
    "        rel_list = list(relevant_set)\n",
    "\n",
    "        # Calculate metrics\n",
    "        p_k = compute_precision_at_K(ranked_pids, rel_list, K)\n",
    "        r_k = compute_recall_at_K(ranked_pids, rel_list, K)\n",
    "        f1_k = compute_F1_score_at_K(ranked_pids, rel_list, K)\n",
    "        ap_k = compute_average_precision_at_K(ranked_pids, rel_list, K)\n",
    "        rr_val = compute_mean_reciprocal_rank([ranked_pids], [rel_list])\n",
    "        ndcg = compute_normalized_discounted_cumulative_gain(ranked_pids, rel_list)\n",
    "\n",
    "        # Print a table for this query\n",
    "        query_metrics[query]['P@K'] = p_k\n",
    "        query_metrics[query]['R@K'] = r_k\n",
    "        query_metrics[query]['F1@K'] = f1_k\n",
    "        query_metrics[query]['AP@K'] = ap_k\n",
    "        query_metrics[query]['RR'] = rr_val\n",
    "        query_metrics[query]['NDCG'] = ndcg\n",
    "\n",
    "    # Print metrics for both queries\n",
    "    for metric_name in ['P@K', 'R@K', 'F1@K', 'AP@K', 'RR', 'NDCG']:\n",
    "        q1_val = query_metrics[queries[0]][metric_name]\n",
    "        q2_val = query_metrics[queries[1]][metric_name]\n",
    "        print(f\"{metric_name:<8} | {q1_val:<18.3f} | {q2_val:<15.3f}\")\n",
    "\n",
    "\n",
    "    map_val = map_at_k(all_results, all_ground_truths, K)\n",
    "    mrr_val = mrr(all_results, all_ground_truths)\n",
    "\n",
    "    print(f\"Overall MAP: {map_val:.3f}\")\n",
    "    print(f\"Overall MRR:    {mrr_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf85622",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

