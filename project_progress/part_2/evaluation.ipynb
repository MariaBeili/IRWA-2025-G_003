{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac44e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170decbf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Import your search functions\n",
    "from project_progress.part_2.indexing import search_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600af472",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_precision_at_K(docs, benchmark, K):\n",
    "    relevant_docs_retrieved = 0\n",
    "\n",
    "    if K == 0 or len(docs) == 0:\n",
    "        return 0.0\n",
    "    elif K > len(docs):\n",
    "        K = len(docs)\n",
    "\n",
    "    for doc in docs[:K]:\n",
    "        if doc in benchmark:\n",
    "            relevant_docs_retrieved += 1\n",
    "\n",
    "    return relevant_docs_retrieved / K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0912c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_recall_at_K(docs, benchmark, K):\n",
    "    relevant_docs_retrieved = 0\n",
    "\n",
    "    if K == 0 or len(benchmark) == 0:\n",
    "        return 0.0\n",
    "    elif K > len(benchmark):\n",
    "        K = len(benchmark)\n",
    "\n",
    "    for doc in benchmark[:K]:\n",
    "        if doc in docs:\n",
    "            relevant_docs_retrieved += 1\n",
    "\n",
    "    return relevant_docs_retrieved / K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed440f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_average_precision_at_K(docs, benchmark, K):\n",
    "    true_positives_seen = 0\n",
    "    index = 0\n",
    "    average_precision = 0\n",
    "\n",
    "    for doc in docs[:K]:\n",
    "        index += 1\n",
    "        if doc in benchmark:\n",
    "            true_positives_seen+=1\n",
    "            precision = true_positives_seen / index\n",
    "            average_precision += precision\n",
    "\n",
    "    if true_positives_seen == 0:\n",
    "        return 0.0\n",
    "    return average_precision / true_positives_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717f8420",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_F1_score_at_K(docs, benchmark, K):\n",
    "    recall = compute_recall_at_K(docs, benchmark, K)\n",
    "    precision = compute_precision_at_K(docs, benchmark, K)\n",
    "    if recall == 0 and precision == 0:\n",
    "        return 0.0\n",
    "    return 2 * recall * precision / (recall + precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb143655",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_mean_average_precision(rankings, benchmarks):\n",
    "    total = 0\n",
    "    for ranking, rel in zip(rankings, benchmarks):\n",
    "        total += compute_average_precision_at_K(ranking, rel, len(ranking))\n",
    "    return total / len(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae44a1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_mean_reciprocal_rank(rankings, benchmarks):\n",
    "    total = 0\n",
    "    for ranking, rel in zip(rankings, benchmarks):\n",
    "        for idx, doc in enumerate(ranking, start=1):\n",
    "            if doc in rel:\n",
    "                total += 1 / idx\n",
    "                break\n",
    "    return total / len(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54fc4e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_normalized_discounted_cumulative_gain(docs, benchmark):\n",
    "    dcg = 0\n",
    "    for i, doc in enumerate(docs, start=1):\n",
    "        if doc in benchmark:\n",
    "            dcg += 1 / math.log2(i + 1)\n",
    "\n",
    "    ideal_relevants = len(benchmark)\n",
    "    idcg = sum(1 / math.log2(i + 1) for i in range(1, ideal_relevants + 1))\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
