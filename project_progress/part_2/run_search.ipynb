{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600dcf57",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the project root to the path so we can import our modules\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b0edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle  # Import pickle to save the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4a373",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Import classes and functions from the other files\n",
    "from project_progress.part_1.data_preparation import ProcessedDocument\n",
    "from project_progress.part_1.data_exploration import parse_numeric, normalize_product_details \n",
    "from project_progress.part_2.indexing import create_index_tfidf, search_tfidf\n",
    "from project_progress.part_2.query_preparation import process_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122d0f0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_and_process_data(data_path: str) -> list[ProcessedDocument]:\n",
    "    \"\"\"Loads the JSON dataset and applies preprocessing to each product.\"\"\"\n",
    "    print(f\"Loading data from: {data_path}\")\n",
    "    df = pd.read_json(data_path)\n",
    "    print(f\"Rows loaded: {len(df)}\")\n",
    "    \n",
    "    processed_docs = []\n",
    "    errors = 0\n",
    "    \n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing documents\"):\n",
    "        try:\n",
    "            rdict = row.to_dict()\n",
    "            \n",
    "            # Clean up some numeric and nested fields\n",
    "            rdict[\"product_details\"] = normalize_product_details(rdict.get(\"product_details\"))\n",
    "            for field in [\"selling_price\", \"actual_price\", \"discount\", \"average_rating\"]:\n",
    "                rdict[field] = parse_numeric(rdict.get(field))\n",
    "\n",
    "            doc_obj = SimpleNamespace(**rdict)\n",
    "            pdoc = ProcessedDocument.from_document(doc_obj)\n",
    "            pdoc.process_fields() \n",
    "            processed_docs.append(pdoc)\n",
    "        except Exception:\n",
    "            errors += 1\n",
    "            continue\n",
    "            \n",
    "    print(f\"Successfully processed: {len(processed_docs)} (Errors: {errors})\")\n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1f57a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Build the index from the dataset\n",
    "    start_time = time.time()\n",
    "    documents = load_and_process_data(\"data/fashion_products_dataset.json\")\n",
    "    \n",
    "    print(\"\\nStarting indexing...\")\n",
    "    index, tf, df, idf, title_index = create_index_tfidf(documents)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"--- Indexing complete! ---\")\n",
    "    print(f\"  Time taken: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"  Vocabulary size: {len(index)} terms\")\n",
    "\n",
    "\n",
    "    # We chose queries that are a bit more specific and realistic.\n",
    "    index_data = {\n",
    "        \"index\": index,\n",
    "        \"tf\": tf,\n",
    "        \"df\": df,\n",
    "        \"idf\": idf,\n",
    "        \"title_index\": title_index\n",
    "    }\n",
    "    index_filename = \"project_progress/part_2/irwa_index.pkl\"\n",
    "    with open(index_filename, \"wb\") as f:\n",
    "        pickle.dump(index_data, f)\n",
    "    print(f\"--- Index saved to {index_filename} ---\")\n",
    "\n",
    "    \n",
    "    my_queries = [\n",
    "        \"ARBO cotton track pants for men\",         \n",
    "        \"Multicolor track pants combo ECKO\",       \n",
    "        \"Black solid women track pants\",      \n",
    "        \"Elastic waist cotton blend track pants\",     \n",
    "        \"Self design multicolor track pants\"  \n",
    "    ]\n",
    "\n",
    "    \n",
    "    print(\"\\n--- Running test queries ---\")\n",
    "    output_filename = \"project_progress/part_2/search_results.txt\"\n",
    "\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for query in my_queries:\n",
    "            search_start = time.time()\n",
    "            ranked_pids = search_tfidf(query, index, tf, idf)\n",
    "            search_end = time.time()\n",
    "\n",
    "            f.write(f\"\\n[Search] Query: '{query}'\\n\")\n",
    "            print(f\"\\n[Search] Query: '{query}'\")\n",
    "\n",
    "            # Also save how the query looks after preprocessing\n",
    "            processed = process_query(query)\n",
    "            f.write(f\"  Processed query: {processed}\\n\")\n",
    "            print(f\"  Processed query: {processed}\")\n",
    "\n",
    "            f.write(f\"  Found {len(ranked_pids)} results in {search_end - search_start:.4f} seconds\\n\")\n",
    "            print(f\"  Found {len(ranked_pids)} results in {search_end - search_start:.4f} seconds\")\n",
    "\n",
    "            if not ranked_pids:\n",
    "                f.write(\"    No results found.\\n\")\n",
    "                print(\"    No results found.\")\n",
    "            else:\n",
    "                for i, pid in enumerate(ranked_pids[:5]):\n",
    "                    title = title_index.get(pid, \"Unknown Title\")\n",
    "                    line = f\"    {i + 1}. (PID: {pid}) {title}\"\n",
    "                    f.write(line + \"\\n\")\n",
    "                    print(line)\n",
    "\n",
    "    print(f\"\\nSearch results saved in {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
