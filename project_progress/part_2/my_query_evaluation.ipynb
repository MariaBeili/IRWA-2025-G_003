{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e46127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the project root to the path so we can import our modules\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b1cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we evaluate our own set of 5 queries against a custom validation set stored in 'my_queries_validation_labels.csv'.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d01983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your TF-IDF search\n",
    "from indexing import search_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99fafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation metrics\n",
    "from evaluation import (\n",
    "    compute_precision_at_K,\n",
    "    compute_recall_at_K,\n",
    "    compute_average_precision_at_K,\n",
    "    compute_F1_score_at_K,\n",
    "    compute_mean_average_precision,\n",
    "    compute_mean_reciprocal_rank,\n",
    "    compute_normalized_discounted_cumulative_gain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d6e84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1dadf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Helper Functions\n",
    "# -------------------------------------------------------------\n",
    "def load_index_data(path: str) -> dict:\n",
    "    # Loads the precomputed inverted index and TF-IDF data\n",
    "    print(f\"Loading index from {path}...\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    print(\"Index loaded successfully.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb92e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8df6a9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_ground_truth(path: str) -> dict:\n",
    "    #Loads my own validation CSV (my_queries_validation_labels.csv)\n",
    "    print(f\"Loading own queries ground truth from {path}...\")\n",
    "    try:\n",
    "        # This CSV is simple: 'query', 'pid', 'labels'\n",
    "        # I created it this way so I don't need the query_id mapping\n",
    "        df = pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Ground truth file not found at {path}\")\n",
    "        print(\"Please create 'my_validation_labels.csv' in the 'data/' folder first.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Group by the query text and get the set of\n",
    "    # relevant PIDs (where 'labels' == 1)\n",
    "    ground_truth = df.groupby('query').apply(\n",
    "        lambda x: set(x[x['labels'] == 1]['pid'])\n",
    "    ).to_dict()\n",
    "\n",
    "    print(f\"Loaded my queries ground truth for {len(ground_truth)} queries.\")\n",
    "    return ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed9876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(all_rankings, all_relevants, K):\n",
    "    \"\"\"Mean Average Precision across queries.\"\"\"\n",
    "    total = 0\n",
    "    for ranking, rel in zip(all_rankings, all_relevants):\n",
    "        total += compute_average_precision_at_K(ranking, rel, K)\n",
    "    return total / len(all_rankings) if all_rankings else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43890b4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def mrr(all_rankings, all_relevants):\n",
    "    \"\"\"Mean Reciprocal Rank across queries.\"\"\"\n",
    "    total = 0\n",
    "    for ranking, rel in zip(all_rankings, all_relevants):\n",
    "        for idx, doc in enumerate(ranking, start=1):\n",
    "            if doc in rel:\n",
    "                total += 1 / idx\n",
    "                break\n",
    "    return total / len(all_rankings) if all_rankings else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc84aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Main Evaluation Script\n",
    "# -------------------------------------------------------------\n",
    "def main():\n",
    "    # File paths and configuration\n",
    "    INDEX_PATH = \"project_progress/part_2/irwa_index.pkl\"\n",
    "    LABELS_PATH = \"data/my_queries_validation_labels.csv\"\n",
    "    K = 10  # Metrics will be computed @10\n",
    "\n",
    "    # Loading data\n",
    "    try:\n",
    "        index_data = load_index_data(INDEX_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Index file not found at {INDEX_PATH}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # We load the index, no need to re-build it\n",
    "    index_data = load_index_data(INDEX_PATH)\n",
    "    ground_truth = load_ground_truth(LABELS_PATH)\n",
    "\n",
    "    index = index_data['index']\n",
    "    tf = index_data['tf']\n",
    "    idf = index_data['idf']\n",
    "\n",
    "    # Queries to evaluate\n",
    "    my_queries = [\n",
    "        \"ARBO cotton track pants for men\",         \n",
    "        \"Multicolor track pants combo ECKO\",       \n",
    "        \"Black solid women track pants\",      \n",
    "        \"Elastic waist cotton blend track pants\",     \n",
    "        \"Self design multicolor track pants\"  \n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    all_ground_truths = []\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Evaluation Results (K={K}) ---\")\n",
    "    print(\"=\" * 44)\n",
    "\n",
    "    for query in my_queries:\n",
    "        ranked_pids = search_tfidf(query, index, tf, idf)\n",
    "        relevant_set = ground_truth.get(query, set())\n",
    "\n",
    "        all_results.append(ranked_pids)\n",
    "        all_ground_truths.append(relevant_set)\n",
    "\n",
    "        # Convert sets to lists because evaluation functions expect ordered lists\n",
    "        rel_list = list(relevant_set)\n",
    "\n",
    "        # Calculate metrics\n",
    "        p_k = compute_precision_at_K(ranked_pids, rel_list, K)\n",
    "        r_k = compute_recall_at_K(ranked_pids, rel_list, K)\n",
    "        f1_k = compute_F1_score_at_K(ranked_pids, rel_list, K)\n",
    "        ap_k = compute_average_precision_at_K(ranked_pids, rel_list, K)\n",
    "        rr_val = compute_mean_reciprocal_rank([ranked_pids], [rel_list])\n",
    "        ndcg = compute_normalized_discounted_cumulative_gain(ranked_pids, rel_list)\n",
    "\n",
    "        # Print a table for this query\n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"  P@10:    {p_k:.3f}\")\n",
    "        print(f\"  R@10:    {r_k:.3f}\")\n",
    "        print(f\"  F1@10:   {f1_k:.3f}\")\n",
    "        print(f\"  AP@10:   {ap_k:.3f}\")\n",
    "        print(f\"  RR:      {rr_val:.3f}\")\n",
    "        print(f\"  NDCG: {ndcg:.3f}\")\n",
    "\n",
    "    # Print metrics for 5 queries\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"--- Overall Results for MY 5 Queries ---\")\n",
    "    \n",
    "    map_val = compute_mean_average_precision(all_results, all_ground_truths)\n",
    "    mrr_val = compute_mean_reciprocal_rank(all_results, all_ground_truths)\n",
    "\n",
    "    print(f\"Overall MAP: {map_val:.3f}\")\n",
    "    print(f\"Overall MRR:    {mrr_val:.3f}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

